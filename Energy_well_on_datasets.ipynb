{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100"
      ],
      "metadata": {
        "id": "y8gJO2u0jlNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import time\n",
        "\n",
        "class GaussianEnergyWellAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(GaussianEnergyWellAttention, self).__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        distances = torch.cdist(query, key, p=2)\n",
        "        weights = torch.exp(-self.alpha * distances ** 2)\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        return torch.matmul(weights, value), weights\n",
        "\n",
        "\n",
        "class SoftmaxExponentialEnergyWellAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SoftmaxExponentialEnergyWellAttention, self).__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        distances = torch.cdist(query, key, p=2)\n",
        "        weights = torch.exp(-self.alpha * distances)\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        return torch.matmul(weights, value), weights\n",
        "\n",
        "\n",
        "class AttentionComparisonModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim, method=\"gaussian\"):\n",
        "        super(AttentionComparisonModel, self).__init__()\n",
        "        self.method = method\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if method == \"gaussian\":\n",
        "            self.attention = GaussianEnergyWellAttention(embedding_dim)\n",
        "        elif method == \"softmax_exponential\":\n",
        "            self.attention = SoftmaxExponentialEnergyWellAttention(embedding_dim)\n",
        "        else:\n",
        "            self.attention = nn.MultiheadAttention(embedding_dim, num_heads=1)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1, self.embedding_dim)\n",
        "        query, key, value = x, x, x\n",
        "        attention_output, _ = self.attention(query, key, value)\n",
        "        pooled_output = attention_output.mean(dim=1)\n",
        "        return self.fc(pooled_output)\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    return accuracy, precision, f1\n",
        "\n",
        "\n",
        "def get_data_loaders(dataset_name):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)) if dataset_name in [\"MNIST\", \"Fashion-MNIST\"] else\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    if dataset_name == \"MNIST\":\n",
        "        dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 10, 28*28\n",
        "    elif dataset_name == \"Fashion-MNIST\":\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 10, 28*28\n",
        "    elif dataset_name == \"CIFAR-10\":\n",
        "        dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 10, 32*32*3\n",
        "    elif dataset_name == \"CIFAR-100\":\n",
        "        dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 100, 32*32*3\n",
        "    elif dataset_name == \"20 Newsgroups\":\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        documents, labels = fetch_20newsgroups(subset='all').data, fetch_20newsgroups(subset='all').target\n",
        "        embeddings = [bert_model(**tokenizer(doc, return_tensors='pt')).last_hidden_state.squeeze(0) for doc in documents]\n",
        "        return embeddings, labels, 768, 20\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader, embedding_dim, output_dim\n",
        "\n",
        "\n",
        "def run_experiment_on_datasets(dataset_list):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    results = []\n",
        "    for dataset_name in dataset_list:\n",
        "        print(f\"\\nRunning experiment on {dataset_name}...\")\n",
        "\n",
        "        train_loader, test_loader, embedding_dim, output_dim = get_data_loaders(dataset_name)\n",
        "\n",
        "        gaussian_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"gaussian\").to(device)\n",
        "        gaussian_optimizer = optim.Adam(gaussian_model.parameters(), lr=0.01)\n",
        "        gauss_accuracy, gauss_precision, gauss_f1 = train_and_evaluate(\n",
        "            gaussian_model, train_loader, test_loader, gaussian_optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        softmax_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"softmax_exponential\").to(device)\n",
        "        softmax_optimizer = optim.Adam(softmax_model.parameters(), lr=0.01)\n",
        "        softmax_accuracy, softmax_precision, softmax_f1 = train_and_evaluate(\n",
        "            softmax_model, train_loader, test_loader, softmax_optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        multihead_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"self_attention\").to(device)\n",
        "        multihead_optimizer = optim.Adam(multihead_model.parameters(), lr=0.01)\n",
        "        multihead_accuracy, multihead_precision, multihead_f1 = train_and_evaluate(\n",
        "            multihead_model, train_loader, test_loader, multihead_optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Gaussian\": {\"Accuracy\": gauss_accuracy, \"Precision\": gauss_precision, \"F1 Score\": gauss_f1},\n",
        "            \"Softmax Exponential\": {\"Accuracy\": softmax_accuracy, \"Precision\": softmax_precision, \"F1 Score\": softmax_f1},\n",
        "            \"Self-Attention\": {\"Accuracy\": multihead_accuracy, \"Precision\": multihead_precision, \"F1 Score\": multihead_f1},\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "dataset_list = [\"MNIST\", \"Fashion-MNIST\", \"CIFAR-10\", \"CIFAR-100\"]\n",
        "\n",
        "results = run_experiment_on_datasets(dataset_list)\n",
        "for result in results:\n",
        "    print(f\"\\nDataset: {result['Dataset']}\")\n",
        "    for method, metrics in result.items():\n",
        "        if method != \"Dataset\":\n",
        "            print(f\"{method} -> Accuracy: {metrics['Accuracy']:.4f}, Precision: {metrics['Precision']:.4f}, F1 Score: {metrics['F1 Score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "wLOERmTSQauF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3d8230-781e-45e2-e405-5232e09f252c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running experiment on MNIST...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running experiment on Fashion-MNIST...\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:05<00:00, 4.49MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 269kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:02<00:00, 2.08MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running experiment on CIFAR-10...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 97.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "\n",
            "Running experiment on CIFAR-100...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 56.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "\n",
            "Dataset: MNIST\n",
            "Gaussian -> Accuracy: 0.8801, Precision: 0.8870, F1 Score: 0.8752\n",
            "Softmax Exponential -> Accuracy: 0.8939, Precision: 0.8986, F1 Score: 0.8938\n",
            "Self-Attention -> Accuracy: 0.1070, Precision: 0.0900, F1 Score: 0.0731\n",
            "\n",
            "Dataset: Fashion-MNIST\n",
            "Gaussian -> Accuracy: 0.8117, Precision: 0.8287, F1 Score: 0.8127\n",
            "Softmax Exponential -> Accuracy: 0.8328, Precision: 0.8381, F1 Score: 0.8340\n",
            "Self-Attention -> Accuracy: 0.1625, Precision: 0.1307, F1 Score: 0.1308\n",
            "\n",
            "Dataset: CIFAR-10\n",
            "Gaussian -> Accuracy: 0.3222, Precision: 0.3286, F1 Score: 0.3201\n",
            "Softmax Exponential -> Accuracy: 0.3182, Precision: 0.3212, F1 Score: 0.3060\n",
            "Self-Attention -> Accuracy: 0.1030, Precision: 0.1043, F1 Score: 0.0969\n",
            "\n",
            "Dataset: CIFAR-100\n",
            "Gaussian -> Accuracy: 0.0848, Precision: 0.0990, F1 Score: 0.0787\n",
            "Softmax Exponential -> Accuracy: 0.0866, Precision: 0.1012, F1 Score: 0.0836\n",
            "Self-Attention -> Accuracy: 0.0116, Precision: 0.0119, F1 Score: 0.0078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations on STL-10, SVNH"
      ],
      "metadata": {
        "id": "Gh9LTCRuj_fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GaussianEnergyWellAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(GaussianEnergyWellAttention, self).__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        distances = torch.cdist(query, key, p=2)\n",
        "        weights = torch.exp(-self.alpha * distances ** 2)\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        return torch.matmul(weights, value), weights\n",
        "\n",
        "\n",
        "class SoftmaxExponentialEnergyWellAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SoftmaxExponentialEnergyWellAttention, self).__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        distances = torch.cdist(query, key, p=2)\n",
        "        weights = torch.exp(-self.alpha * distances)\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        return torch.matmul(weights, value), weights\n",
        "\n",
        "class AttentionComparisonModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim, method=\"gaussian\"):\n",
        "        super(AttentionComparisonModel, self).__init__()\n",
        "        self.method = method\n",
        "        self.embedding_dim = embedding_dim\n",
        "        if method == \"gaussian\":\n",
        "            self.attention = GaussianEnergyWellAttention(embedding_dim)\n",
        "        elif method == \"softmax_exponential\":\n",
        "            self.attention = SoftmaxExponentialEnergyWellAttention(embedding_dim)\n",
        "        else:\n",
        "            self.attention = nn.MultiheadAttention(embedding_dim, num_heads=1)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        if self.method in [\"gaussian\", \"softmax_exponential\"]:\n",
        "            x = x.view(batch_size, -1, self.embedding_dim)\n",
        "            query, key, value = x, x, x\n",
        "            attention_output, _ = self.attention(query, key, value)\n",
        "            pooled_output = attention_output.mean(dim=1)\n",
        "        else:\n",
        "            x = x.view(batch_size, -1, self.embedding_dim).permute(1, 0, 2)\n",
        "            query, key, value = x, x, x\n",
        "            attention_output, _ = self.attention(query, key, value)\n",
        "            pooled_output = attention_output.mean(dim=0)\n",
        "\n",
        "        return self.fc(pooled_output)\n",
        "\n",
        "\n",
        "def get_data_loaders(dataset_name, batch_size=64):\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    if dataset_name == \"STL-10\":\n",
        "        train_dataset = datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
        "        test_dataset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 10, 96*96*3\n",
        "    elif dataset_name == \"SVHN\":\n",
        "        train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "        test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "        output_dim, embedding_dim = 10, 32*32*3\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, embedding_dim, output_dim\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * (np.array(y_true) == np.array(y_pred)).sum() / len(y_true)\n",
        "    precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    return accuracy, precision, f1\n",
        "\n",
        "def run_experiment_on_datasets(dataset_list):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    results = []\n",
        "\n",
        "    for dataset_name in dataset_list:\n",
        "        train_loader, test_loader, embedding_dim, output_dim = get_data_loaders(dataset_name)\n",
        "\n",
        "        gaussian_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"gaussian\").to(device)\n",
        "        gaussian_optimizer = optim.Adam(gaussian_model.parameters(), lr=0.001)\n",
        "        gauss_accuracy, gauss_precision, gauss_f1 = train_and_evaluate(gaussian_model, train_loader, test_loader, gaussian_optimizer, criterion, device)\n",
        "\n",
        "        softmax_exponential_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"softmax_exponential\").to(device)\n",
        "        softmax_exponential_optimizer = optim.Adam(softmax_exponential_model.parameters(), lr=0.001)\n",
        "        softmax_accuracy, softmax_precision, softmax_f1 = train_and_evaluate(softmax_exponential_model, train_loader, test_loader, softmax_exponential_optimizer, criterion, device)\n",
        "\n",
        "        conventional_model = AttentionComparisonModel(embedding_dim, output_dim, method=\"self_attention\").to(device)\n",
        "        conventional_optimizer = optim.Adam(conventional_model.parameters(), lr=0.001)\n",
        "        conv_accuracy, conv_precision, conv_f1 = train_and_evaluate(conventional_model, train_loader, test_loader, conventional_optimizer, criterion, device)\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Gaussian (Acc, Prec, F1)\": (gauss_accuracy, gauss_precision, gauss_f1),\n",
        "            \"Softmax Exponential (Acc, Prec, F1)\": (softmax_accuracy, softmax_precision, softmax_f1),\n",
        "            \"Self-Attention (Acc, Prec, F1)\": (conv_accuracy, conv_precision, conv_f1),\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "dataset_list = [\"STL-10\", \"SVHN\"]\n",
        "results = run_experiment_on_datasets(dataset_list)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"\\nDataset: {result['Dataset']}\")\n",
        "    print(f\"  Gaussian Energy Well Attention - Accuracy: {result['Gaussian (Acc, Prec, F1)'][0]:.2f}%, Precision: {result['Gaussian (Acc, Prec, F1)'][1]:.2f}, F1 Score: {result['Gaussian (Acc, Prec, F1)'][2]:.2f}\")\n",
        "    print(f\"  Softmax Exponential Attention - Accuracy: {result['Softmax Exponential (Acc, Prec, F1)'][0]:.2f}%, Precision: {result['Softmax Exponential (Acc, Prec, F1)'][1]:.2f}, F1 Score: {result['Softmax Exponential (Acc, Prec, F1)'][2]:.2f}\")\n",
        "    print(f\"  Self-Attention - Accuracy: {result['Self-Attention (Acc, Prec, F1)'][0]:.2f}%, Precision: {result['Self-Attention (Acc, Prec, F1)'][1]:.2f}, F1 Score: {result['Self-Attention (Acc, Prec, F1)'][2]:.2f}\")"
      ],
      "metadata": {
        "id": "VDohj7NLxP8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}